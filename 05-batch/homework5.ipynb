{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import IntegerType\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/03/06 20:42:54 WARN Utils: Your hostname, localhost resolves to a loopback address: 127.0.0.1; using 192.168.1.101 instead (on interface en0)\n",
      "25/03/06 20:42:54 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/03/06 20:42:55 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName('test') \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.5.1'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df = spark.read.parquet('yellow_tripdata_2024-10.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "main_dir = 'yellow_2024_04'\n",
    "df = df.repartition(4)\n",
    "df.write.mode(\"overwrite\").parquet(main_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22.381834983825684\n",
      "22.396079063415527\n",
      "22.42396831512451\n",
      "22.36391258239746\n"
     ]
    }
   ],
   "source": [
    "parquet_files = [f for f in os.listdir(main_dir) if f.endswith('.parquet')]\n",
    "for i in parquet_files:\n",
    "    size_mb = os.path.getsize(f'{main_dir}/{i}') / (1024 * 1024)\n",
    "    print(size_mb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|   cnt|\n",
      "+------+\n",
      "|128893|\n",
      "+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.createOrReplaceTempView(\"trips\")\n",
    "spark.sql(\"\"\"SELECT count(1) as cnt\n",
    "             from trips\n",
    "             WHERE DATE(tpep_pickup_datetime)  = '2024-10-15'\n",
    "         \"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 10:==============>                                           (1 + 3) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------------+\n",
      "|VendorID|         trip_hour|\n",
      "+--------+------------------+\n",
      "|       2|162.61777777777777|\n",
      "+--------+------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_unix = df \\\n",
    "            .withColumn('pickup_unix', F.unix_timestamp(df.tpep_pickup_datetime) ) \\\n",
    "            .withColumn('dropoff_unix', F.unix_timestamp(df.tpep_dropoff_datetime) )\n",
    "df_trip_dur = df_unix \\\n",
    "                    .withColumn('trip_hour', (df_unix.dropoff_unix - df_unix.pickup_unix) / 3600)\n",
    "df_trip_dur.createOrReplaceTempView(\"trips_dur\")\n",
    "spark.sql(\"\"\"\n",
    "          SELECT VendorID, trip_hour\n",
    "          FROM trips_dur\n",
    "          ORDER BY trip_hour DESC\n",
    "          LIMIT 1\n",
    "          \"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lookup = spark.read.option(\"header\", \"true\").csv('taxi_zone_lookup.csv')\n",
    "df_lookup_clean = df_lookup \\\n",
    "                        .withColumn(\"lo_id\", F.col(\"LocationID\").cast(IntegerType()) )\n",
    "df_lookup_clean.createOrReplaceTempView(\"lookup\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 17:>                                                         (0 + 4) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------------+--------+\n",
      "|zone                                         |count(1)|\n",
      "+---------------------------------------------+--------+\n",
      "|Governor's Island/Ellis Island/Liberty Island|1       |\n",
      "+---------------------------------------------+--------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "         SELECT l.zone, COUNT(*)\n",
    "         FROM trips as t\n",
    "         LEFT JOIN lookup as l\n",
    "         ON t.PULocationID = l.lo_id\n",
    "         GROUP BY l.zone\n",
    "         ORDER BY COUNT(*)\n",
    "         LIMIT 1\n",
    "         \"\"\").show(truncate = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
